---
title: "MVT vs. MVN random field differences"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
---

In this document we will simulate some data that comes from a MVT random field model and then fit models that allow for the MVT random field or assume the standard MVN Gaussian random field. 

```{r set-knitr-options, cache=FALSE, echo=FALSE}
library("knitr")
opts_chunk$set(message=FALSE, fig.width=7, fig.height=5, cache = FALSE, autodep = TRUE)
```
  
```{r}
library(rrfields)
library(dplyr)
library(ggplot2)
library(rstan)
library(bayesplot)
```

Let's initialize some argument and parameter values that we will use throughout.

```{r setup}
options(mc.cores = parallel::detectCores())
ITER <- 600
CHAINS <- 2
SEED <- 1
gp_sigma <- 0.3
sigma <- 0.8
df <- 2
gp_scale <- 1.2
n_draws <- 16
nknots <- 14
```

Let's simulate some data that has heavy tails. 

```{r simulate-data}
set.seed(SEED)
s <- sim_rrfield(df = df, n_draws = n_draws, gp_scale = gp_scale,
  gp_sigma = gp_sigma, sd_obs = sigma, n_knots = nknots)
print(s$plot)
```

We set the degrees of freedom parameter to 2. This represents data about have some very heavy tails. Each of the above panels refers to a different slice of time. We can see that some time slices have some very extreme spatial patterns where the peaks and valleys are quite large. Time slices would be more similar to each other if the degrees of freedom parameter were larger. 

Let's fit a model where we estimate the degrees of freedom parameter. 

```{r, message=FALSE, results=FALSE, message=FALSE}
m1 <- rrfield(y ~ 0, data = s$dat, time = "time", station = "station_id",
  lat = "lat", lon = "lon", nknots = nknots,
  iter = ITER, chains = CHAINS, estimate_df = TRUE, save_log_lik = TRUE)
# print(m1)
```

So we have recovered the degrees of freedom parameter. 

Let's look at the coverage of our correct model. 

```{r}
p <- predict(m1, type = "response")
pp <- predict(m1, interval = "prediction", type = "response")
plot(s$dat$y, p$estimate, col = "#00000060")
segments(s$dat$y, pp$conf_low, s$dat$y, pp$conf_high, lwd = 0.5, col = "#00000090")
segments(s$dat$y, p$conf_low, s$dat$y, p$conf_high, lwd = 1.5)
abline(a = 0, b = 1, lty = 2)

(coverage <- mean(s$dat$y > pp$conf_low & s$dat$y < pp$conf_high) %>% round(3))
```

In the above plot the true values are on the x-axis and the estimates are on the y-axis. The darker vertical lines indicate the 95% credible intervals on the mean predictions and the longer fainter vertical lines indicate the 95% prediction credible intervals. These longer lines include the observation model. 

The coverage of our prediction intervals is approximately correct. 

Now let's fit a model where we force the random field to be multivariate normal by fixing the degrees of freedom parameter at a large value. 

```{r, message=FALSE, results=FALSE, message=FALSE}
m_wrong <- rrfield(y ~ 0, data = s$dat, time = "time", station = "station_id",
  lat = "lat", lon = "lon", nknots = nknots,
  iter = ITER, chains = CHAINS,
  estimate_df = FALSE, fixed_df_value = 1e6, save_log_lik = TRUE)
# print(m_wrong)
```

Let's look at the difference between the parameter estimates.

```{r}
pars <- c("df[1]", "gp_sigma", "sigma[1]", "gp_scale")
mm <- as.matrix(m1$model)
p1 <- mcmc_areas(mm, pars = pars) + xlim(0, 5)

pars <- c("gp_sigma", "sigma[1]", "gp_scale")
mm_wrong <- as.matrix(m_wrong$model)
p2 <- mcmc_areas(mm_wrong, pars = pars) + xlim(0, 5)

gridExtra::grid.arrange(p1, p2)
```

So to compensate, the MVN model (the mismatched model; the one on the bottom), has allowed the gp_sigma parameter to inflate. This parameter controls the scale of the spatial deviations.

Let's calculate the same credible intervals for the mismatched model. 

```{r}
p_wrong <- predict(m_wrong, type = "response")
pp_wrong <- predict(m_wrong, interval = "prediction", type = "response")
plot(s$dat$y, p_wrong$estimate, col = "#00000060")
segments(s$dat$y, pp_wrong$conf_low, s$dat$y, pp_wrong$conf_high, lwd = 0.5, col = "#00000090")
segments(s$dat$y, p_wrong$conf_low, s$dat$y, p_wrong$conf_high, lwd = 1.5)
abline(a = 0, b = 1, lty = 2)

mean(s$dat$y > pp_wrong$conf_low & s$dat$y < pp_wrong$conf_high) %>% round(3)
```

So coverage isn't a problem. It is also approximately correct for the mismatched model. We will see that the model generates similar coverage by inflating the credible intervals in order to make up for slightly poorer prediction when the models are mismatched. 

Let's look at the ratio of the credible interval widths. The objects starting with `p` contain the credible intervals on the mean and the objects starting with `pp` contain the prediction credible intervals (i.e. posterior predictive checks).

```{r}
cis_wrong <- p_wrong$conf_high - p_wrong$conf_low
cis <- p$conf_high - p$conf_low

ggplot(data.frame(y = s$dat$y, cis = cis, ratio = exp(log(cis_wrong) - log(cis)), 
  time = as.factor(s$dat$time)), 
  aes(as.factor(time), ratio)) +
  geom_boxplot() +
  geom_hline(yintercept = 1, lty = 2) +
  coord_flip()
```

So here we can see that the credible intervals are considerably wider for the mismatched model. They are often twenty to sixty percent larger in this case. 

Let's look at the sum of the squared residuals:

```{r}
mean((p$estimate - s$dat$y)^2)
mean((p_wrong$estimate - s$dat$y)^2)
```

So the sum of the squared residuals are also larger for the mismatched model. 

Now let's combine the predictions to compare them:

```{r}
d <- data.frame(s$dat, p)
d_wrong <- data.frame(s$dat, p_wrong)
d_combined <- data.frame(d, select(d_wrong, estimate) %>% rename(est_wrong = estimate))
```

The following plot looks at the differences in the predictions spatially 

```{r}
ggplot(d_combined, aes(lon, lat, colour = estimate - est_wrong)) +
  geom_point(size = 2) +
  scale_color_gradient2() +
  facet_wrap(~time)
```

Let's graphically look at the coverage of the 2 models. We will show the first 15 points.

```{r, fig.width=10, fig.height=10}
proj <- reshape2::melt(s$proj)
names(proj) <- c("time", "pt", "proj")
proj <- dplyr::arrange_(proj, "time", "pt")

d2 <- data.frame(d_combined, proj = proj$proj)
d2 <- data.frame(d2, rename(d_wrong, conf_low_wrong = conf_low, conf_high_wrong = conf_high) %>% 
    select(conf_low_wrong, conf_high_wrong))
jitter <- 0.25

filter(d2, pt %in% 1:15) %>%
  ggplot(aes(x = conf_low, xend = conf_high, y = pt+jitter, yend = pt+jitter)) +
  geom_segment(alpha = 0.5) +
  geom_segment(aes(x = conf_low_wrong, xend = conf_high_wrong, 
    y = pt-jitter, yend = pt-jitter), alpha = 0.5, colour = "red") +
  geom_point(aes(x = proj, y = pt)) +
  geom_point(aes(x = estimate, y = pt+jitter), pch = 4) +
  geom_point(aes(x = est_wrong, y = pt-jitter), colour = "red", pch = 4) +
  facet_wrap(~ time, scales = "free_x") + theme_light() +
  xlab("y") + ylab("Point ID")
```

So in the above figure, the black circles are the true values and the x's and horizontal lines represent the median model estimates and 95% credible intervals. The black lines represent the correct MVT model and the red represents the mismatched MVN model. 

In order to capture the time slices with big changes in the spatial pattern, the MVN model increases the gp_sigma parameter, which means that years with more normal special deviations are estimated as being overly variable. The credible intervals become larger as well. For example, look at the larger deviations in panel 13. In this time slice the mismatched model overestimates how large the largest deviations are and "compensates" with larger credible intervals.

Let's look at the root mean squared error for the 2 models.

```{r}
ms1 <- group_by(d2, time) %>%
  summarize(rmse_wrong = sqrt(mean((est_wrong - proj) ^ 2)),
  rmse = sqrt(mean((estimate - proj) ^ 2)))
 
d2 %>%
  summarize(rmse_wrong = sqrt(mean((est_wrong - proj) ^ 2)),
  rmse = sqrt(mean((estimate - proj) ^ 2)))

ggplot(ms1, aes(time, rmse)) +
  geom_line() +
  geom_line(aes(y = rmse_wrong), col = "red")
```

So the multivariate model almost always has the same or higher root mean square error.

Perhaps a better way to compare the models incorporating the 4 posterior and not just point estimates as with a posterior log score.

```{r, eval=TRUE}
p <- predict(m1, return_mcmc = TRUE)
scores <- matrix(0, nrow = nrow(p), ncol = ncol(p))
sigma <- extract(m1$model)$sigma
for(draw in seq_len(ncol(p))){
  scores[, draw] <- dnorm(d$y,
    mean = p[, draw], sd = sigma[draw])
}

p_wrong <- predict(m_wrong, return_mcmc = TRUE)
sigma_wrong <- extract(m_wrong$model)$sigma
scores_wrong <- matrix(0, nrow = nrow(p), ncol = ncol(p))
for(draw in seq_len(ncol(p))){
  scores_wrong[, draw] <- dnorm(d$y,
    mean = p_wrong[, draw], sd = sigma_wrong[draw])
}

-mean(log(rowMeans(1/scores)))
-mean(log(rowMeans(1/scores_wrong)))
```

We can look at the leave-one-out information criteria to compare the models.

```{r}
library(loo)
loo_t <- loo(extract_log_lik(m1$model))
loo_n <- loo(extract_log_lik(m_wrong$model))
loo_t$looic
loo_n$looic
```

Indeed the leave one out information criteria favors the correct MVT model here.

The gold standard is cross validation. 

```{r}
set.seed(1)

s$dat$leave_out <- ifelse(s$dat$pt %in% sample(1:100, 10), TRUE, FALSE)

m1 <- rrfield(y ~ 0, data = filter(s$dat, !leave_out), time = "time", station = "station_id",
  lat = "lat", lon = "lon", nknots = nknots,
  iter = ITER, chains = CHAINS, estimate_df = TRUE)

m_wrong <- rrfield(y ~ 0, data = filter(s$dat, !leave_out), time = "time", station = "station_id",
  lat = "lat", lon = "lon", nknots = nknots,
  iter = ITER, chains = CHAINS,
  estimate_df = FALSE, fixed_df_value = 1e9)

d1 <- filter(s$dat, leave_out)
d1$pred <- predict(m1, newdata = filter(s$dat, leave_out))
d1$pred_wrong <- predict(m_wrong, newdata = filter(s$dat, leave_out))

sqrt(mean((d1$pred_wrong - d1$y) ^ 2))
sqrt(mean((d1$pred - d1$y) ^ 2))



p <- predict(m1, return_mcmc = TRUE)
scores <- matrix(0, nrow = nrow(p), ncol = ncol(p))
sigma <- extract(m1$model)$sigma
for(draw in seq_len(ncol(p))){
  scores[, draw] <- dnorm(d$y,
    mean = p[, draw], sd = sigma[draw])
}

p_wrong <- predict(m_wrong, return_mcmc = TRUE)
sigma_wrong <- extract(m_wrong$model)$sigma
scores_wrong <- matrix(0, nrow = nrow(p), ncol = ncol(p))
for(draw in seq_len(ncol(p))){
  scores_wrong[, draw] <- dnorm(d$y,
    mean = p_wrong[, draw], sd = sigma_wrong[draw])
}

-mean(log(rowMeans(1/scores)))
-mean(log(rowMeans(1/scores_wrong)))
```

