---
title: "MVT vs. MVN performance for true versus observed values"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
---

```{r set-knitr-options, cache=FALSE, echo=FALSE}
library("knitr")
opts_chunk$set(message=FALSE, fig.width=7, fig.height=5, cache = FALSE, autodep = TRUE)
```
  
```{r}
library(rrfields)
library(dplyr)
library(ggplot2)
library(rstan)
```

Let's initialize some argument and parameter values that we will use throughout.

```{r setup}
options(mc.cores = parallel::detectCores())
ITER <- 400
CHAINS <- 2
SEED <- 1
gp_sigma <- 0.3
sigma <- 0.6
df <- 2
gp_scale <- 1.2
n_draws <- 15
nknots <- 10
```

Let's simulate some data that has heavy tails, fit correct and mismatched models, make predictions, and then calculate the root mean squared error compared to the true values.

The following function will accomplish that. 

```{r simulate-and-fit-data}
simulate_and_fit <- function() {
  s <- sim_rrfield(df = df, n_draws = n_draws, gp_scale = gp_scale,
    gp_sigma = gp_sigma, sd_obs = sigma, n_knots = nknots)
  
  d <- s$dat
  d <- mutate(d, withhold = station_id %in% sample(unique(station_id), 10))
  
  m1 <- rrfield(y ~ 0, data = filter(d, !withhold), 
    time = "time", station = "station_id",
    lat = "lat", lon = "lon", nknots = nknots,
    iter = ITER, chains = CHAINS, estimate_df = TRUE)
  
  m_wrong <- rrfield(y ~ 0, data = filter(d, !withhold), 
    time = "time", station = "station_id",
    lat = "lat", lon = "lon", nknots = nknots,
    iter = ITER, chains = CHAINS,
    estimate_df = FALSE, fixed_df_value = 1e6)
  
  diag <- broom::tidy(m1$model, rhat = TRUE, ess = TRUE) %>% select(rhat, ess)
  diagw <- broom::tidy(m_wrong$model, rhat = TRUE, ess = TRUE) %>% select(rhat, ess)
  
  assertthat::assert_that(max(diag$rhat) < 1.05)
  assertthat::assert_that(max(diagw$rhat) < 1.05)
  assertthat::assert_that(max(diag$ess) > 100)
  assertthat::assert_that(max(diagw$ess) > 100)
  
  p <- predict(m1, type = "response", newdata = filter(d, withhold))
  p_wrong <- predict(m_wrong, newdata = filter(d, withhold), type = "response")
  
  out <- data.frame(filter(d, withhold), p)
  out_wrong <- data.frame(filter(d, withhold), p_wrong)
  d_combined <- data.frame(out, select(out_wrong, estimate) %>% 
      rename(est_wrong = estimate))
  
  proj <- reshape2::melt(s$proj)
  names(proj) <- c("time", "pt", "proj")
  proj <- dplyr::arrange_(proj, "time", "pt")
  d2 <- data.frame(d_combined, proj = proj$proj)
  
  rmse <- d2 %>% summarize(rmse_wrong = sqrt(mean((est_wrong - proj) ^ 2)),
    rmse = sqrt(mean((estimate - proj) ^ 2)))
  
  rmse
}
```

Now let's run our function a number of times:

```{r}
set.seed(SEED)
output <- plyr::rdply(.n = 1, simulate_and_fit)
output
```

The first figure shows the distribution of root mean squared error for the various runs. The labels "_y" indicate the models compared to the data and the once without "_y" are compared to the true values. 

The last 2 figures look at the ratio of correctly specified root mean squared error to the incorrectly specified model root mean squared error. 

```{r}
reshape2::melt(output, ".n") %>% 
  mutate(type = grepl("y", variable)) %>%
  ggplot(aes(variable, value)) +
  geom_boxplot() + 
  facet_wrap(~type, scales = "free")

output %>%
  ggplot(aes(0, rmse / rmse_wrong)) + geom_point()

output %>%
  ggplot(aes(0, rmse_y / rmse_wrong_y)) + geom_point()
```

So what is the difference? Let's look at the parameter estimates. 

```{r}
# median(extract(m1$model)$sigma)
# median(extract(m_wrong$model)$sigma)
# 
# extract(m1$model)$sigma %>% median()
# extract(m_wrong$model)$sigma %>% median()
```

